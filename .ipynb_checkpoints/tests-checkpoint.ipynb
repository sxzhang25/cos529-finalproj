{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from oneshot import *\n",
    "\n",
    "def initialize_W(a, size):\n",
    "  '''\n",
    "  initialize random weight matrix according to normalized initialization\n",
    "  http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
    "  '''\n",
    "  W = np.random.uniform(-a, a, size=size)\n",
    "  return W\n",
    "\n",
    "def initialize_b(size):\n",
    "  '''\n",
    "  initialize bias vector\n",
    "  '''\n",
    "  return np.zeros((size,))\n",
    "\n",
    "def create_model(n_visible, n_hidden1, n_hidden2):\n",
    "  '''\n",
    "  creates a two-layer deep boltzmann machine\n",
    "\n",
    "  matrix notation pulled from:temp\n",
    "  http://proceedings.mlr.press/v5/tempsalakhutdinov09a/salakhutdinov09a.pdf\n",
    "  '''\n",
    "  model = {}\n",
    "\n",
    "  a = np.sqrt(6 / n_visible + n_hidden1) # initialization factor\n",
    "  W = initialize_W(a, (n_visible, n_hidden1)) # visible-to-hidden weight matrices\n",
    "  b_w = initialize_b(n_hidden1) # bias vector\n",
    "  a = np.sqrt(6 / n_hidden1 + n_hidden2);\n",
    "  J = initialize_W(a, (n_hidden1, n_hidden2)) # hidden-to-hidden weigh matrices\n",
    "  b_j = initialize_b(n_hidden2) # bias vector\n",
    "\n",
    "  # initialize persistent markov chains\n",
    "  b_v = initialize_b(n_visible)\n",
    "  b_h1 = initialize_b(n_hidden1)\n",
    "  b_h2 = initialize_b(n_hidden2)\n",
    "\n",
    "  # set model\n",
    "  model['W'] = W\n",
    "  model['b_w'] = b_w\n",
    "  model['J'] = J\n",
    "  model['b_j'] = b_j\n",
    "  model['b_v'] = b_v\n",
    "  model['b_h1'] = b_h1\n",
    "  model['b_h2'] = b_h2\n",
    "\n",
    "  return model\n",
    "\n",
    "def sigmoid(X):\n",
    "  return 1 / (1 + np.exp(-X))\n",
    "\n",
    "def cross_entropy(X, model):\n",
    "  W, J, b_v = model['W'], model['J'], model['b_v']\n",
    "  h2 = np.random.rand(J.shape[1], X.shape[1])\n",
    "  h1, _, _ = gibbs_sampling(h2, h2, X, model, 1)\n",
    "  v = sigmoid(W @ h1 + np.array([b_v,] * X.shape[1]).T) \n",
    "  cross_entropy = -np.mean(np.sum(X * np.log(v) + (1 - X) * np.log(1 - v), axis=0))\n",
    "  return cross_entropy\n",
    "\n",
    "def mean_field_update(X, model, delta):\n",
    "  W, J = model['W'], model['J']\n",
    "  b_w, b_j = model['b_w'], model['b_j']\n",
    "  mu1 = np.random.rand(J.shape[0], X.shape[1])\n",
    "  mu2 = np.random.rand(J.shape[1], X.shape[1])\n",
    "  while True:\n",
    "    mu1_ = sigmoid(W.T @ X + J @ mu2 + np.array([b_w,] * X.shape[1]).T)\n",
    "    mu2_ = sigmoid(J.T @ mu1 + np.array([b_j,] * X.shape[1]).T)\n",
    "\n",
    "    delta_mu1 = np.linalg.norm(mu1_ - mu1)\n",
    "    delta_mu2 = np.linalg.norm(mu2_ - mu2)\n",
    "\n",
    "    mu1, mu2 = mu1_, mu2_\n",
    "    if (delta_mu1 < delta and delta_mu2 < delta):\n",
    "      break\n",
    "\n",
    "  return mu1, mu2\n",
    "\n",
    "def gibbs_sampling(h1, h2, v, model, n_steps):\n",
    "  W, J = model['W'], model['J']\n",
    "  b_v, b_h1, b_h2 = model['b_v'], model['b_h1'], model['b_h2']\n",
    "  for i in range(n_steps):\n",
    "    h1 = sigmoid(W.T @ v + J @ h2 + np.array([b_h1,] * v.shape[1]).T)\n",
    "    h1 = np.random.binomial(1, h1)\n",
    "    h2 = sigmoid(J.T @ h1 + np.array([b_h2,] * v.shape[1]).T)\n",
    "    h2 = np.random.binomial(1, h2)\n",
    "    v = sigmoid(W @ h1 + np.array([b_v,] * v.shape[1]).T)\n",
    "    v = np.random.binomial(1, v)\n",
    "  return h1, h2, v\n",
    "\n",
    "def train(model, X_train, n_epochs=10, K=100, batch_size=32, mf_delta=1, gibbs_steps=2, lr=0.001):\n",
    "  np.random.seed(0)\n",
    "  W, J = model['W'], model['J']\n",
    "\n",
    "  train_error = []\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "    # calculate error\n",
    "    error = cross_entropy(X_train, model)\n",
    "    train_error.append(error)\n",
    "    print(\"Epoch %d: train error: %2.2f\" % (epoch, error))\n",
    "\n",
    "    # persistent markov chains\n",
    "    n_hidden1, n_hidden2 = model['J'].shape\n",
    "    v = np.random.binomial(1, 0.5, (X_train.shape[0], K))\n",
    "    h1 = np.random.binomial(1, 0.5, (n_hidden1, K))\n",
    "    h2 = np.random.binomial(1, 0.5, (n_hidden2, K))\n",
    "\n",
    "    for i in range(X_train.shape[1] // batch_size):\n",
    "      batch = X_train[:, np.random.permutation(X_train.shape[1])[:batch_size]]\n",
    "\n",
    "      mu1, mu2 = mean_field_update(batch, model, mf_delta)\n",
    "      h1, h2, v = gibbs_sampling(h1, h2, v, model, gibbs_steps)\n",
    "\n",
    "      h1_batch = sigmoid(W.T @ batch + J @ mu2 + np.array([b_h1,] * batch.shape[1]).T)\n",
    "      h1_v = sigmoid(W.T @ v + J @ h2 + np.array([b_h1,] * batch.shape[1]).T)\n",
    "      h2_batch = sigmoid(J.T @ mu1 + np.array([b_h2,] * batch.shape[1]).T)\n",
    "      h2_v = sigmoid(J.T @ h1 + np.array([b_h2,] * batch.shape[1]).T)\n",
    "\n",
    "      # update weights\n",
    "      W += lr * (batch.dot(mu1.T) / batch.shape[1] - v.dot(h1.T) / v.shape[1])\n",
    "      J += lr * (mu1.dot(mu2.T) / mu1.shape[1] - h1.dot(h2.T) / h1.shape[1])\n",
    "      b_v += lr * (np.sum(batch, axis=1, keepdims=True) / batch.shape[1] - np.sum(v, axis=1, keepdims=True) / v.shape[1])\n",
    "      b_h1 += lr * (np.sum(h1_batch, axis=1, keepdims=True) / h1_batch.shape[1] - np.sum(h1_v, axis=1, keepdims=True) / h1_v.shape[1])\n",
    "      b_h2 += lr * (np.sum(h2_batch, axis=1, keepdims=True) / h2_batch.shape[1] - np.sum(h2_v, axis=1, keepdims=True) / h2_v.shape[1])\n",
    "\n",
    "      lr *= 0.9\n",
    "\n",
    "  return train_error, model\n",
    "\n",
    "# def test_oneshot(model, N, k, data, labels, alphabet_dict, language=None, verbose=0):\n",
    "#   '''\n",
    "#   Test average N-way oneshot learning accuracy of model over k one-shot tasks\n",
    "#   '''\n",
    "#   correct = 0\n",
    "#   if verbose:\n",
    "#     print(\"Evaluating model on {} random {}-way one-shot learning tasks...\".format(k,N))\n",
    "#\n",
    "#   for i in range(k):\n",
    "#     inputs, targets = create_oneshot_task(data, labels, alphabet_dict, N=N, language=language)\n",
    "#     probs = model.predict(inputs)\n",
    "#     if np.argmax(probs) == np.argmax(targets):\n",
    "#       correct += 1\n",
    "#\n",
    "#   accuracy = (100 * correct / k)\n",
    "#   if verbose:\n",
    "#     print(\"Average %d-way one-shot accuracy: %4.2f%%\" % (N, accuracy))\n",
    "#\n",
    "#   return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.004969867177253281\n"
     ]
    }
   ],
   "source": [
    "# X_train = np.loadtxt('data/dbm_train.dat')\n",
    "# print(X_train.shape)\n",
    "print(X_train[0][147])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/szhang25/miniconda3/envs/cos429/lib/python3.7/site-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train error: inf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-6c47bab501c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdbm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-68-9b151f12e526>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, X_train, n_epochs, K, batch_size, mf_delta, gibbs_steps, lr)\u001b[0m\n\u001b[1;32m    110\u001b[0m       \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m       \u001b[0mmu1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_field_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmf_delta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m       \u001b[0mh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgibbs_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgibbs_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-9b151f12e526>\u001b[0m in \u001b[0;36mmean_field_update\u001b[0;34m(X, model, delta)\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0mmu2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mmu1_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mJ\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mmu2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_w\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mmu2_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mmu1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_j\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dbm_model = create_model(X_train.shape[0], 100, 200)\n",
    "train(dbm_model, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
