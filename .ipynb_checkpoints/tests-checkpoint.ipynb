{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from mnist import MNIST\n",
    "\n",
    "import naive_nn as nnn\n",
    "import twin_nn as tnn\n",
    "from preprocessing import *\n",
    "from oneshot import *\n",
    "\n",
    "np.random.seed(0) # set seed\n",
    "\n",
    "import keras\n",
    "\n",
    "twin_nn = keras.models.load_model('models/twin_nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading alphabet: Asomtavruli_(Georgian)\n",
      "Loading alphabet: Malay_(Jawi_-_Arabic)\n",
      "Loading alphabet: Braille\n",
      "Loading alphabet: Armenian\n",
      "Loading alphabet: N_Ko\n",
      "Loading alphabet: Early_Aramaic\n",
      "Loading alphabet: Tagalog\n",
      "Loading alphabet: Cyrillic\n",
      "Loading alphabet: Burmese_(Myanmar)\n",
      "Loading alphabet: Syriac_(Estrangelo)\n",
      "Loading alphabet: Greek\n",
      "Loading alphabet: Japanese_(katakana)\n",
      "Loading alphabet: Ojibwe_(Canadian_Aboriginal_Syllabics)\n",
      "Loading alphabet: Hebrew\n",
      "Loading alphabet: Alphabet_of_the_Magi\n",
      "Loading alphabet: Latin\n",
      "Loading alphabet: Mkhedruli_(Georgian)\n",
      "Loading alphabet: Japanese_(hiragana)\n",
      "Loading alphabet: Balinese\n",
      "Loading alphabet: Arcadian\n",
      "Loading alphabet: Bengali\n",
      "Loading alphabet: Gujarati\n",
      "Loading alphabet: Grantha\n",
      "Loading alphabet: Sanskrit\n",
      "Loading alphabet: Tifinagh\n",
      "Loading alphabet: Blackfoot_(Canadian_Aboriginal_Syllabics)\n",
      "Loading alphabet: Anglo-Saxon_Futhorc\n",
      "Loading alphabet: Futurama\n",
      "Loading alphabet: Inuktitut_(Canadian_Aboriginal_Syllabics)\n",
      "Loading alphabet: Korean\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 105, 105, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 105, 105, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 4096)         38947648    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 4096)         0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            4097        lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 38,951,745\n",
      "Trainable params: 38,951,745\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "603/602 [==============================] - 108s 179ms/step - loss: 2.9173 - binary_accuracy: 0.5332\n",
      "Epoch 2/10\n",
      "603/602 [==============================] - 107s 177ms/step - loss: 1.3936 - binary_accuracy: 0.5338\n",
      "Epoch 3/10\n",
      "603/602 [==============================] - 107s 177ms/step - loss: 1.0491 - binary_accuracy: 0.5419\n",
      "Epoch 4/10\n",
      "603/602 [==============================] - 107s 177ms/step - loss: 0.8482 - binary_accuracy: 0.6093\n",
      "Epoch 5/10\n",
      "603/602 [==============================] - 107s 177ms/step - loss: 0.7523 - binary_accuracy: 0.6465\n",
      "Epoch 6/10\n",
      "603/602 [==============================] - 107s 177ms/step - loss: 0.6902 - binary_accuracy: 0.6800\n",
      "Epoch 7/10\n",
      "603/602 [==============================] - 107s 177ms/step - loss: 0.6454 - binary_accuracy: 0.7074\n",
      "Epoch 8/10\n",
      "603/602 [==============================] - 107s 177ms/step - loss: 0.6137 - binary_accuracy: 0.7354\n",
      "Epoch 9/10\n",
      "603/602 [==============================] - 107s 177ms/step - loss: 0.6004 - binary_accuracy: 0.7447\n",
      "Epoch 10/10\n",
      "603/602 [==============================] - 107s 177ms/step - loss: 0.5736 - binary_accuracy: 0.7666\n"
     ]
    }
   ],
   "source": [
    "# tests\n",
    "tests = ['omniglot']\n",
    "\n",
    "for test in tests:\n",
    "  if test == 'basic':\n",
    "    anchors = np.array([[1,1], [1,-1], [-1,1], [-1,-1]])\n",
    "    labels = np.array([0, 0, 1, 2], dtype='int')\n",
    "    data = 2 * np.random.rand(500, 2) - 1\n",
    "\n",
    "    naive_nn = nnn.Naive_NN(anchors, labels)\n",
    "    y_ = naive_nn.classify(data)\n",
    "\n",
    "    naive_nn.plot()\n",
    "\n",
    "  elif test == 'mnist':\n",
    "    mndata = MNIST('data/mnist')\n",
    "    scaler = StandardScaler()\n",
    "    cpc = 2  # centers per class\n",
    "\n",
    "    # prepare anchors\n",
    "    anchors, anchor_labels = mndata.load_training()\n",
    "    anchor_labels = np.array(anchor_labels)\n",
    "\n",
    "    anchor_idxs = np.zeros((0,), dtype='int')\n",
    "    for i in range(10):\n",
    "      idxs = np.where(anchor_labels==i)[0]\n",
    "      new_idxs = np.array([idxs[i] for i in np.random.randint(0, len(idxs), cpc)])\n",
    "      anchor_idxs = np.concatenate((anchor_idxs, new_idxs), axis=0)\n",
    "\n",
    "    anchors = np.array([anchors[idx] for idx in anchor_idxs]).T\n",
    "    anchor_labels = np.array([anchor_labels[idx] for idx in anchor_idxs])\n",
    "\n",
    "    test_data, test_labels = mndata.load_testing()  # load test data\n",
    "    test_data = np.array(test_data).T\n",
    "    test_labels = np.array(test_labels)\n",
    "\n",
    "    index = np.random.randint(0, len(anchor_labels))  # choose an index\n",
    "    print(mndata.display(anchors.T[index]))\n",
    "\n",
    "    pca = PCA(n_components=2, svd_solver='arpack')\n",
    "    pca.fit(np.hstack((anchors, test_data)))\n",
    "    anchors_, test_data_ = pca.components_.T[:10 * cpc], pca.components_.T[10 * cpc:]\n",
    "\n",
    "    naive_nn = nnn.Naive_NN(anchors_, anchor_labels)\n",
    "    y_ = naive_nn.classify(test_data_)\n",
    "\n",
    "    naive_nn.plot()\n",
    "\n",
    "    print('\\nAccuracy: %2.2f %%' % (100 * len(np.where(y_==test_labels)[0]) / len(y_)))\n",
    "\n",
    "  elif test == 'omniglot':\n",
    "    X, y, alphabet_dict, char_dict = load_imgs('./data/omniglot/images_background')\n",
    "    n_classes, n_examples, w, h = X.shape\n",
    "\n",
    "    twin_nn = tnn.create_model((w,h,1))\n",
    "    \n",
    "    optimizer = Adam(lr=0.00006)\n",
    "    twin_nn.compile(loss='binary_crossentropy',\n",
    "                    optimizer=optimizer,\n",
    "                    metrics=['binary_accuracy'])\n",
    "\n",
    "    twin_nn.summary()\n",
    "\n",
    "    batch_size = 32\n",
    "    history = twin_nn.fit_generator(generator=tnn.training_generator(X, batch_size=batch_size),\n",
    "                                  steps_per_epoch=(X.shape[0] * X.shape[1] / batch_size),\n",
    "                                  epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "603/602 [==============================] - 107s 178ms/step - loss: 0.5588 - binary_accuracy: 0.7772\n",
      "Epoch 2/10\n",
      "603/602 [==============================] - 106s 177ms/step - loss: 0.5432 - binary_accuracy: 0.7875\n",
      "Epoch 3/10\n",
      "603/602 [==============================] - 106s 177ms/step - loss: 0.5309 - binary_accuracy: 0.8060\n",
      "Epoch 4/10\n",
      "603/602 [==============================] - 106s 177ms/step - loss: 0.4780 - binary_accuracy: 0.8495\n",
      "Epoch 5/10\n",
      "603/602 [==============================] - 107s 177ms/step - loss: 0.4096 - binary_accuracy: 0.8947\n",
      "Epoch 6/10\n",
      "603/602 [==============================] - 106s 176ms/step - loss: 0.3709 - binary_accuracy: 0.9127\n",
      "Epoch 7/10\n",
      "603/602 [==============================] - 106s 176ms/step - loss: 0.3443 - binary_accuracy: 0.9277\n",
      "Epoch 8/10\n",
      "603/602 [==============================] - 106s 177ms/step - loss: 0.3261 - binary_accuracy: 0.9342\n",
      "Epoch 9/10\n",
      "603/602 [==============================] - 106s 176ms/step - loss: 0.2986 - binary_accuracy: 0.9482\n",
      "Epoch 10/10\n",
      "603/602 [==============================] - 106s 176ms/step - loss: 0.2969 - binary_accuracy: 0.9468\n"
     ]
    }
   ],
   "source": [
    "history = twin_nn.fit_generator(generator=tnn.training_generator(X, batch_size=batch_size),\n",
    "                                  steps_per_epoch=(X.shape[0] * X.shape[1] / batch_size),\n",
    "                                  epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'twin_nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-077e751b77a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtwin_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/twin_nn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'twin_nn' is not defined"
     ]
    }
   ],
   "source": [
    "twin_nn.save('models/twin_nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "twin_nn = keras.models.load_model('models/twin_nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading alphabet: Sylheti\n",
      "Loading alphabet: Oriya\n",
      "Loading alphabet: Angelic\n",
      "Loading alphabet: Old_Church_Slavonic_(Cyrillic)\n",
      "Loading alphabet: Mongolian\n",
      "Loading alphabet: Kannada\n",
      "Loading alphabet: Keble\n",
      "Loading alphabet: Syriac_(Serto)\n",
      "Loading alphabet: Manipuri\n",
      "Loading alphabet: Atlantean\n",
      "Loading alphabet: Gurmukhi\n",
      "Loading alphabet: Avesta\n",
      "Loading alphabet: Glagolitic\n",
      "Loading alphabet: Tengwar\n",
      "Loading alphabet: Atemayar_Qelisayer\n",
      "Loading alphabet: Malayalam\n",
      "Loading alphabet: Ge_ez\n",
      "Loading alphabet: Tibetan\n",
      "Loading alphabet: Aurek-Besh\n",
      "Loading alphabet: ULOG\n"
     ]
    }
   ],
   "source": [
    "# get test data\n",
    "test_X, test_y, test_alphabet_dict, _ = load_imgs(\n",
    "  './data/omniglot/images_evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_oneshot(twin_nn, 10, 100, test_X, test_y, test_alphabet_dict, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
